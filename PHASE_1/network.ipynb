{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Preprocessing\n",
    "You'll first need to preprocess the CSV data to create the appropriate format for training the DBN. This involves separating the data into features and targets for training and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Timestep  Projectile_Position_X  Projectile_Position_Y  \\\n",
      "0        0.000000               0.000000               0.000000   \n",
      "1        0.311655             690.425786             237.256245   \n",
      "2        0.623311            1380.851572             473.559653   \n",
      "3        0.934966            2071.277357             708.910225   \n",
      "4        1.246622            2761.703143             943.307961   \n",
      "...           ...                    ...                    ...   \n",
      "74994  110.702338          348760.080941            5381.196688   \n",
      "74995  110.926432          349466.073007            4313.671839   \n",
      "74996  111.150525          350172.065074            3241.789745   \n",
      "74997  111.374619          350878.057141            2165.550408   \n",
      "74998  111.598713          351584.049207            1084.953826   \n",
      "\n",
      "       Projectile_Velocity  Projectile_Angle  Interceptor_Position_X  \\\n",
      "0              2342.502760         18.964647           400000.000000   \n",
      "1              2342.502760         18.964647           399545.459165   \n",
      "2              2341.510956         18.893896           399090.918330   \n",
      "3              2340.522727         18.823085           398636.377495   \n",
      "4              2339.538075         18.752215           398181.836660   \n",
      "...                    ...               ...                     ...   \n",
      "74994          1914.942704        -56.414015           121277.540173   \n",
      "74995          1920.392578        -56.521921           120713.324668   \n",
      "74996          1925.849225        -56.629216           120149.109162   \n",
      "74997          1931.312587        -56.735905           119584.893656   \n",
      "74998          1936.782607        -56.841991           119020.678150   \n",
      "\n",
      "       Interceptor_Position_Y  Interceptor_Velocity  Interceptor_Angle  \n",
      "0                    0.000000           2999.804704         165.283936  \n",
      "1                  119.382774           2999.804704         165.283936  \n",
      "2                  238.524779           2999.414666         165.312331  \n",
      "3                  357.426017           2999.025364         165.340733  \n",
      "4                  476.086485           2998.636799         165.369142  \n",
      "...                       ...                   ...                ...  \n",
      "74994           120468.419228           2576.355842         167.757321  \n",
      "74995           120590.354299           2575.890569         167.805107  \n",
      "74996           120711.796731           2575.427089         167.852910  \n",
      "74997           120832.746524           2574.965402         167.900731  \n",
      "74998           120953.203679           2574.505510         167.948569  \n",
      "\n",
      "[74999 rows x 9 columns]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "With n_samples=0, test_size=0.2 and train_size=None, the resulting train set will be empty. Adjust any of the aforementioned parameters.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 62\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m X_group_t\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m==\u001b[39m y_group_t\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFeature and target matrices have different lengths within group\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;66;03m# Split within the group\u001b[39;00m\n\u001b[1;32m---> 62\u001b[0m X_train, X_test, y_train, y_test \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_test_split\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_group_t\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_group_t\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m42\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;66;03m# Append to combined lists\u001b[39;00m\n\u001b[0;32m     65\u001b[0m X_train_combined\u001b[38;5;241m.\u001b[39mappend(X_train)\n",
      "File \u001b[1;32mc:\\Users\\suvar\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_split.py:2562\u001b[0m, in \u001b[0;36mtrain_test_split\u001b[1;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[0m\n\u001b[0;32m   2559\u001b[0m arrays \u001b[38;5;241m=\u001b[39m indexable(\u001b[38;5;241m*\u001b[39marrays)\n\u001b[0;32m   2561\u001b[0m n_samples \u001b[38;5;241m=\u001b[39m _num_samples(arrays[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m-> 2562\u001b[0m n_train, n_test \u001b[38;5;241m=\u001b[39m \u001b[43m_validate_shuffle_split\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2563\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_samples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdefault_test_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.25\u001b[39;49m\n\u001b[0;32m   2564\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2566\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m shuffle \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[0;32m   2567\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m stratify \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\suvar\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_split.py:2236\u001b[0m, in \u001b[0;36m_validate_shuffle_split\u001b[1;34m(n_samples, test_size, train_size, default_test_size)\u001b[0m\n\u001b[0;32m   2233\u001b[0m n_train, n_test \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(n_train), \u001b[38;5;28mint\u001b[39m(n_test)\n\u001b[0;32m   2235\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_train \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m-> 2236\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   2237\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWith n_samples=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, test_size=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m and train_size=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2238\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresulting train set will be empty. Adjust any of the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2239\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maforementioned parameters.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(n_samples, test_size, train_size)\n\u001b[0;32m   2240\u001b[0m     )\n\u001b[0;32m   2242\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m n_train, n_test\n",
      "\u001b[1;31mValueError\u001b[0m: With n_samples=0, test_size=0.2 and train_size=None, the resulting train set will be empty. Adjust any of the aforementioned parameters."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "# Load the data\n",
    "csv_file = r'C:\\Users\\suvar\\General\\Work\\Varsity\\Honours\\Research\\Lab\\PHASE_1\\data\\synthetic_trajectory_data.csv'\n",
    "data = pd.read_csv(csv_file)\n",
    "\n",
    "# Extract features and targets\n",
    "features = ['Timestep', 'Projectile_Position_X', 'Projectile_Position_Y',\n",
    "            'Projectile_Velocity', 'Projectile_Angle',\n",
    "            'Interceptor_Position_X', 'Interceptor_Position_Y',\n",
    "            'Interceptor_Velocity', 'Interceptor_Angle']\n",
    "\n",
    "# Features for current timestep\n",
    "X = data[features].copy()\n",
    "\n",
    "# Add future timestep data to create the target variables\n",
    "future_features = ['Projectile_Position_X', 'Projectile_Position_Y',\n",
    "                    'Interceptor_Position_X', 'Interceptor_Position_Y']\n",
    "y = data[future_features].shift(-1).copy()  # Shift by one timestep to get future positions\n",
    "\n",
    "# Drop NaN values\n",
    "X = X[:-1]  # Remove last row from X\n",
    "y = y.dropna().reset_index(drop=True)  # Remove rows with NaN values from y and reset index\n",
    "print(X)\n",
    "# Ensure X and y have the same length\n",
    "assert X.shape[0] == y.shape[0], \"Feature and target matrices have different lengths\"\n",
    "\n",
    "# Add time slice information to columns\n",
    "def add_time_slices(df, time_slice):\n",
    "    df.columns = [f'{col}_{time_slice}' for col in df.columns]\n",
    "    return df\n",
    "\n",
    "# Group by 'Trajectory_ID' and split within each group\n",
    "grouped = data.groupby('Trajectory_ID')\n",
    "X_train_combined = []\n",
    "X_test_combined = []\n",
    "y_train_combined = []\n",
    "y_test_combined = []\n",
    "\n",
    "for name, group in grouped:\n",
    "    # Extract features and targets with time slices\n",
    "    time_slices = group['Timestep'].unique()\n",
    "    for t in time_slices:\n",
    "        group_t = group[group['Timestep'] == t]\n",
    "        X_group_t = group_t[features].copy()\n",
    "        y_group_t = group_t[future_features].shift(-1).copy().dropna()\n",
    "        \n",
    "        # Add time slices to the columns\n",
    "        X_group_t = add_time_slices(X_group_t, t)\n",
    "        y_group_t = add_time_slices(y_group_t, t)\n",
    "\n",
    "        # Drop NaN values\n",
    "        X_group_t = X_group_t[:-1]\n",
    "        y_group_t = y_group_t.dropna().reset_index(drop=True)\n",
    "\n",
    "        # Ensure X and y have the same length within the group\n",
    "        assert X_group_t.shape[0] == y_group_t.shape[0], \"Feature and target matrices have different lengths within group\"\n",
    "\n",
    "        # Split within the group\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X_group_t, y_group_t, test_size=0.2, random_state=42)\n",
    "\n",
    "        # Append to combined lists\n",
    "        X_train_combined.append(X_train)\n",
    "        X_test_combined.append(X_test)\n",
    "        y_train_combined.append(y_train)\n",
    "        y_test_combined.append(y_test)\n",
    "\n",
    "# Concatenate all the splits\n",
    "X_train_combined = pd.concat(X_train_combined, axis=0).reset_index(drop=True)\n",
    "X_test_combined = pd.concat(X_test_combined, axis=0).reset_index(drop=True)\n",
    "y_train_combined = pd.concat(y_train_combined, axis=0).reset_index(drop=True)\n",
    "y_test_combined = pd.concat(y_test_combined, axis=0).reset_index(drop=True)\n",
    "\n",
    "print(f\"Shapes - X_train_combined: {X_train_combined.shape}, y_train_combined: {y_train_combined.shape}\")\n",
    "print(f\"Shapes - X_test_combined: {X_test_combined.shape}, y_test_combined: {y_test_combined.shape}\")\n",
    "\n",
    "# Save or use the data for DBN fitting\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define and Train the DBN\n",
    "Update the DBN structure to reflect the temporal dependencies and relationships for trajectory prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "data column names must start from time slice 0.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 31\u001b[0m\n\u001b[0;32m     28\u001b[0m dbn \u001b[38;5;241m=\u001b[39m define_dbn_structure()\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m# Fit the DBN using the MaximumLikelihoodEstimator\u001b[39;00m\n\u001b[1;32m---> 31\u001b[0m \u001b[43mdbn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mestimator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mMaximumLikelihoodEstimator\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\suvar\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pgmpy\\models\\DynamicBayesianNetwork.py:866\u001b[0m, in \u001b[0;36mDynamicBayesianNetwork.fit\u001b[1;34m(self, data, estimator)\u001b[0m\n\u001b[0;32m    863\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata must be a pandas dataframe. Got: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(data)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    865\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mmin\u001b[39m(data\u001b[38;5;241m.\u001b[39mcolumns, key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m t: t[\u001b[38;5;241m1\u001b[39m])[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 866\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata column names must start from time slice 0.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    868\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m estimator \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMLE\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmle\u001b[39m\u001b[38;5;124m\"\u001b[39m}:\n\u001b[0;32m    869\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOnly Maximum Likelihood Estimator is supported currently\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: data column names must start from time slice 0."
     ]
    }
   ],
   "source": [
    "from pgmpy.models import DynamicBayesianNetwork as DBN\n",
    "from pgmpy.estimators import MaximumLikelihoodEstimator\n",
    "\n",
    "def define_dbn_structure():\n",
    "    dbn = DBN()\n",
    "    \n",
    "    # Temporal edges\n",
    "    edges = [\n",
    "        (('Projectile_Position_X', 0), ('Projectile_Position_X', 1)),\n",
    "        (('Projectile_Position_Y', 0), ('Projectile_Position_Y', 1)),\n",
    "        (('Interceptor_Position_X', 0), ('Interceptor_Position_X', 1)),\n",
    "        (('Interceptor_Position_Y', 0), ('Interceptor_Position_Y', 1))\n",
    "    ]\n",
    "    dbn.add_edges_from(edges)\n",
    "    \n",
    "    # Dependencies between current state variables\n",
    "    dependencies = [\n",
    "        (('Projectile_Position_X', 0), ('Interceptor_Position_X', 0)),\n",
    "        (('Projectile_Position_Y', 0), ('Interceptor_Position_Y', 0)),\n",
    "        (('Interceptor_Position_X', 0), ('Interceptor_Position_X', 1)),\n",
    "        (('Interceptor_Position_Y', 0), ('Interceptor_Position_Y', 1))\n",
    "    ]\n",
    "    dbn.add_edges_from(dependencies)\n",
    "    \n",
    "    return dbn\n",
    "\n",
    "# Initialize the DBN structure\n",
    "dbn = define_dbn_structure()\n",
    "\n",
    "# Fit the DBN using the MaximumLikelihoodEstimator\n",
    "dbn.fit(X_train, estimator=MaximumLikelihoodEstimator)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict and Evaluate\n",
    "Use the trained DBN to predict the future positions and compare them with the actual future positions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pgmpy.inference import DBNInference\n",
    "\n",
    "# Initialize the DBN Inference\n",
    "dbn_infer = DBNInference(dbn)\n",
    "\n",
    "# Predict and evaluate\n",
    "predictions = []\n",
    "for i in range(len(X_test)):\n",
    "    evidence = X_test.iloc[i].to_dict()\n",
    "    pred = dbn_infer.forward_inference(variables=[('Projectile_Position_X', 1), ('Projectile_Position_Y', 1),\n",
    "                                                  ('Interceptor_Position_X', 1), ('Interceptor_Position_Y', 1)],\n",
    "                                        evidence=evidence)\n",
    "    predictions.append(pred)\n",
    "\n",
    "# Convert predictions to DataFrame for comparison\n",
    "predictions_df = pd.DataFrame(predictions)\n",
    "\n",
    "# Evaluate the performance (you may need to adapt this based on your exact evaluation criteria)\n",
    "accuracy = np.mean(np.abs(predictions_df.values - y_test.values) < tolerance)\n",
    "print(f\"Prediction Accuracy: {accuracy:.2f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
